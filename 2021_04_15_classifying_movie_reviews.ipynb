{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "2021_04_15-classifying-movie-reviews.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeoneunjae/prac_class/blob/main/2021_04_15_classifying_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS89nQUCcCt0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42f3de05-c5e3-496e-c334-11c691ea76a5"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4OaTI1OcCt8"
      },
      "source": [
        "# Classifying movie reviews: a binary classification example\n",
        "\n",
        "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we \n",
        "will learn to classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kIk9u-tcCt9"
      },
      "source": [
        "## The IMDB dataset\n",
        "\n",
        "\n",
        "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 \n",
        "reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
        "\n",
        "Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to \n",
        "train it! Just because a model performs well on its training data doesn't mean that it will perform well on data it has never seen, and \n",
        "what you actually care about is your model's performance on new data (since you already know the labels of your training data -- obviously \n",
        "you don't need your model to predict those). For instance, it is possible that your model could end up merely _memorizing_ a mapping between \n",
        "your training samples and their targets -- which would be completely useless for the task of predicting targets for data never seen before. \n",
        "We will go over this point in much more detail in the next chapter.\n",
        "\n",
        "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
        "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
        "\n",
        "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52S7dYz-cCt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e70b1b-23c3-4ce7-f1dd-554d20a1f3f6"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "#In the imdb dataset, we can load data using 'imdb.load_data()'ftn.\n",
        "#num_words=10000 means that we will only keep the top 10,000 most frequently occurring words in the training data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqbRodZXcCt9"
      },
      "source": [
        "\n",
        "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words \n",
        "will be discarded. This allows us to work with vector data of manageable size.\n",
        "\n",
        "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
        "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDKRPacFcCt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3372e96-39f4-4bd0-9b05-3663495fdb3a"
      },
      "source": [
        "train_data[0]\n",
        "#리뷰 텍스트는 어휘 사전의 특정 단어를 나타내는 정수로 변환되어 있음.\n",
        "#위 코드는 리뷰 텍스트의 첫 번째 리뷰를 확인하는 코드임."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_fYhzftcCt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf99b4fc-8058-4f84-a4f0-609eb42ca463"
      },
      "source": [
        "train_labels[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZgky6wqcCt-"
      },
      "source": [
        "Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUR6f1YAcCt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98606ef2-c400-4058-c2cd-97b0a1221b78"
      },
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dXpD67jcCt_"
      },
      "source": [
        "For kicks, here's how you can quickly decode one of these reviews back to English words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tTxL-edcCt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c47cd1-eeae-43a3-88c7-f2c611e4f5d5"
      },
      "source": [
        "# word_index is a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "# We reverse it, mapping integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# We decode the review; note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVNzeLK57mrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c3541e-ec90-4fb1-eff6-4da87a084963"
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fawn': 34701,\n",
              " 'tsukino': 52006,\n",
              " 'nunnery': 52007,\n",
              " 'sonja': 16816,\n",
              " 'vani': 63951,\n",
              " 'woods': 1408,\n",
              " 'spiders': 16115,\n",
              " 'hanging': 2345,\n",
              " 'woody': 2289,\n",
              " 'trawling': 52008,\n",
              " \"hold's\": 52009,\n",
              " 'comically': 11307,\n",
              " 'localized': 40830,\n",
              " 'disobeying': 30568,\n",
              " \"'royale\": 52010,\n",
              " \"harpo's\": 40831,\n",
              " 'canet': 52011,\n",
              " 'aileen': 19313,\n",
              " 'acurately': 52012,\n",
              " \"diplomat's\": 52013,\n",
              " 'rickman': 25242,\n",
              " 'arranged': 6746,\n",
              " 'rumbustious': 52014,\n",
              " 'familiarness': 52015,\n",
              " \"spider'\": 52016,\n",
              " 'hahahah': 68804,\n",
              " \"wood'\": 52017,\n",
              " 'transvestism': 40833,\n",
              " \"hangin'\": 34702,\n",
              " 'bringing': 2338,\n",
              " 'seamier': 40834,\n",
              " 'wooded': 34703,\n",
              " 'bravora': 52018,\n",
              " 'grueling': 16817,\n",
              " 'wooden': 1636,\n",
              " 'wednesday': 16818,\n",
              " \"'prix\": 52019,\n",
              " 'altagracia': 34704,\n",
              " 'circuitry': 52020,\n",
              " 'crotch': 11585,\n",
              " 'busybody': 57766,\n",
              " \"tart'n'tangy\": 52021,\n",
              " 'burgade': 14129,\n",
              " 'thrace': 52023,\n",
              " \"tom's\": 11038,\n",
              " 'snuggles': 52025,\n",
              " 'francesco': 29114,\n",
              " 'complainers': 52027,\n",
              " 'templarios': 52125,\n",
              " '272': 40835,\n",
              " '273': 52028,\n",
              " 'zaniacs': 52130,\n",
              " '275': 34706,\n",
              " 'consenting': 27631,\n",
              " 'snuggled': 40836,\n",
              " 'inanimate': 15492,\n",
              " 'uality': 52030,\n",
              " 'bronte': 11926,\n",
              " 'errors': 4010,\n",
              " 'dialogs': 3230,\n",
              " \"yomada's\": 52031,\n",
              " \"madman's\": 34707,\n",
              " 'dialoge': 30585,\n",
              " 'usenet': 52033,\n",
              " 'videodrome': 40837,\n",
              " \"kid'\": 26338,\n",
              " 'pawed': 52034,\n",
              " \"'girlfriend'\": 30569,\n",
              " \"'pleasure\": 52035,\n",
              " \"'reloaded'\": 52036,\n",
              " \"kazakos'\": 40839,\n",
              " 'rocque': 52037,\n",
              " 'mailings': 52038,\n",
              " 'brainwashed': 11927,\n",
              " 'mcanally': 16819,\n",
              " \"tom''\": 52039,\n",
              " 'kurupt': 25243,\n",
              " 'affiliated': 21905,\n",
              " 'babaganoosh': 52040,\n",
              " \"noe's\": 40840,\n",
              " 'quart': 40841,\n",
              " 'kids': 359,\n",
              " 'uplifting': 5034,\n",
              " 'controversy': 7093,\n",
              " 'kida': 21906,\n",
              " 'kidd': 23379,\n",
              " \"error'\": 52041,\n",
              " 'neurologist': 52042,\n",
              " 'spotty': 18510,\n",
              " 'cobblers': 30570,\n",
              " 'projection': 9878,\n",
              " 'fastforwarding': 40842,\n",
              " 'sters': 52043,\n",
              " \"eggar's\": 52044,\n",
              " 'etherything': 52045,\n",
              " 'gateshead': 40843,\n",
              " 'airball': 34708,\n",
              " 'unsinkable': 25244,\n",
              " 'stern': 7180,\n",
              " \"cervi's\": 52046,\n",
              " 'dnd': 40844,\n",
              " 'dna': 11586,\n",
              " 'insecurity': 20598,\n",
              " \"'reboot'\": 52047,\n",
              " 'trelkovsky': 11037,\n",
              " 'jaekel': 52048,\n",
              " 'sidebars': 52049,\n",
              " \"sforza's\": 52050,\n",
              " 'distortions': 17633,\n",
              " 'mutinies': 52051,\n",
              " 'sermons': 30602,\n",
              " '7ft': 40846,\n",
              " 'boobage': 52052,\n",
              " \"o'bannon's\": 52053,\n",
              " 'populations': 23380,\n",
              " 'chulak': 52054,\n",
              " 'mesmerize': 27633,\n",
              " 'quinnell': 52055,\n",
              " 'yahoo': 10307,\n",
              " 'meteorologist': 52057,\n",
              " 'beswick': 42577,\n",
              " 'boorman': 15493,\n",
              " 'voicework': 40847,\n",
              " \"ster'\": 52058,\n",
              " 'blustering': 22922,\n",
              " 'hj': 52059,\n",
              " 'intake': 27634,\n",
              " 'morally': 5621,\n",
              " 'jumbling': 40849,\n",
              " 'bowersock': 52060,\n",
              " \"'porky's'\": 52061,\n",
              " 'gershon': 16821,\n",
              " 'ludicrosity': 40850,\n",
              " 'coprophilia': 52062,\n",
              " 'expressively': 40851,\n",
              " \"india's\": 19500,\n",
              " \"post's\": 34710,\n",
              " 'wana': 52063,\n",
              " 'wang': 5283,\n",
              " 'wand': 30571,\n",
              " 'wane': 25245,\n",
              " 'edgeways': 52321,\n",
              " 'titanium': 34711,\n",
              " 'pinta': 40852,\n",
              " 'want': 178,\n",
              " 'pinto': 30572,\n",
              " 'whoopdedoodles': 52065,\n",
              " 'tchaikovsky': 21908,\n",
              " 'travel': 2103,\n",
              " \"'victory'\": 52066,\n",
              " 'copious': 11928,\n",
              " 'gouge': 22433,\n",
              " \"chapters'\": 52067,\n",
              " 'barbra': 6702,\n",
              " 'uselessness': 30573,\n",
              " \"wan'\": 52068,\n",
              " 'assimilated': 27635,\n",
              " 'petiot': 16116,\n",
              " 'most\\x85and': 52069,\n",
              " 'dinosaurs': 3930,\n",
              " 'wrong': 352,\n",
              " 'seda': 52070,\n",
              " 'stollen': 52071,\n",
              " 'sentencing': 34712,\n",
              " 'ouroboros': 40853,\n",
              " 'assimilates': 40854,\n",
              " 'colorfully': 40855,\n",
              " 'glenne': 27636,\n",
              " 'dongen': 52072,\n",
              " 'subplots': 4760,\n",
              " 'kiloton': 52073,\n",
              " 'chandon': 23381,\n",
              " \"effect'\": 34713,\n",
              " 'snugly': 27637,\n",
              " 'kuei': 40856,\n",
              " 'welcomed': 9092,\n",
              " 'dishonor': 30071,\n",
              " 'concurrence': 52075,\n",
              " 'stoicism': 23382,\n",
              " \"guys'\": 14896,\n",
              " \"beroemd'\": 52077,\n",
              " 'butcher': 6703,\n",
              " \"melfi's\": 40857,\n",
              " 'aargh': 30623,\n",
              " 'playhouse': 20599,\n",
              " 'wickedly': 11308,\n",
              " 'fit': 1180,\n",
              " 'labratory': 52078,\n",
              " 'lifeline': 40859,\n",
              " 'screaming': 1927,\n",
              " 'fix': 4287,\n",
              " 'cineliterate': 52079,\n",
              " 'fic': 52080,\n",
              " 'fia': 52081,\n",
              " 'fig': 34714,\n",
              " 'fmvs': 52082,\n",
              " 'fie': 52083,\n",
              " 'reentered': 52084,\n",
              " 'fin': 30574,\n",
              " 'doctresses': 52085,\n",
              " 'fil': 52086,\n",
              " 'zucker': 12606,\n",
              " 'ached': 31931,\n",
              " 'counsil': 52088,\n",
              " 'paterfamilias': 52089,\n",
              " 'songwriter': 13885,\n",
              " 'shivam': 34715,\n",
              " 'hurting': 9654,\n",
              " 'effects': 299,\n",
              " 'slauther': 52090,\n",
              " \"'flame'\": 52091,\n",
              " 'sommerset': 52092,\n",
              " 'interwhined': 52093,\n",
              " 'whacking': 27638,\n",
              " 'bartok': 52094,\n",
              " 'barton': 8775,\n",
              " 'frewer': 21909,\n",
              " \"fi'\": 52095,\n",
              " 'ingrid': 6192,\n",
              " 'stribor': 30575,\n",
              " 'approporiately': 52096,\n",
              " 'wobblyhand': 52097,\n",
              " 'tantalisingly': 52098,\n",
              " 'ankylosaurus': 52099,\n",
              " 'parasites': 17634,\n",
              " 'childen': 52100,\n",
              " \"jenkins'\": 52101,\n",
              " 'metafiction': 52102,\n",
              " 'golem': 17635,\n",
              " 'indiscretion': 40860,\n",
              " \"reeves'\": 23383,\n",
              " \"inamorata's\": 57781,\n",
              " 'brittannica': 52104,\n",
              " 'adapt': 7916,\n",
              " \"russo's\": 30576,\n",
              " 'guitarists': 48246,\n",
              " 'abbott': 10553,\n",
              " 'abbots': 40861,\n",
              " 'lanisha': 17649,\n",
              " 'magickal': 40863,\n",
              " 'mattter': 52105,\n",
              " \"'willy\": 52106,\n",
              " 'pumpkins': 34716,\n",
              " 'stuntpeople': 52107,\n",
              " 'estimate': 30577,\n",
              " 'ugghhh': 40864,\n",
              " 'gameplay': 11309,\n",
              " \"wern't\": 52108,\n",
              " \"n'sync\": 40865,\n",
              " 'sickeningly': 16117,\n",
              " 'chiara': 40866,\n",
              " 'disturbed': 4011,\n",
              " 'portmanteau': 40867,\n",
              " 'ineffectively': 52109,\n",
              " \"duchonvey's\": 82143,\n",
              " \"nasty'\": 37519,\n",
              " 'purpose': 1285,\n",
              " 'lazers': 52112,\n",
              " 'lightened': 28105,\n",
              " 'kaliganj': 52113,\n",
              " 'popularism': 52114,\n",
              " \"damme's\": 18511,\n",
              " 'stylistics': 30578,\n",
              " 'mindgaming': 52115,\n",
              " 'spoilerish': 46449,\n",
              " \"'corny'\": 52117,\n",
              " 'boerner': 34718,\n",
              " 'olds': 6792,\n",
              " 'bakelite': 52118,\n",
              " 'renovated': 27639,\n",
              " 'forrester': 27640,\n",
              " \"lumiere's\": 52119,\n",
              " 'gaskets': 52024,\n",
              " 'needed': 884,\n",
              " 'smight': 34719,\n",
              " 'master': 1297,\n",
              " \"edie's\": 25905,\n",
              " 'seeber': 40868,\n",
              " 'hiya': 52120,\n",
              " 'fuzziness': 52121,\n",
              " 'genesis': 14897,\n",
              " 'rewards': 12607,\n",
              " 'enthrall': 30579,\n",
              " \"'about\": 40869,\n",
              " \"recollection's\": 52122,\n",
              " 'mutilated': 11039,\n",
              " 'fatherlands': 52123,\n",
              " \"fischer's\": 52124,\n",
              " 'positively': 5399,\n",
              " '270': 34705,\n",
              " 'ahmed': 34720,\n",
              " 'zatoichi': 9836,\n",
              " 'bannister': 13886,\n",
              " 'anniversaries': 52127,\n",
              " \"helm's\": 30580,\n",
              " \"'work'\": 52128,\n",
              " 'exclaimed': 34721,\n",
              " \"'unfunny'\": 52129,\n",
              " '274': 52029,\n",
              " 'feeling': 544,\n",
              " \"wanda's\": 52131,\n",
              " 'dolan': 33266,\n",
              " '278': 52133,\n",
              " 'peacoat': 52134,\n",
              " 'brawny': 40870,\n",
              " 'mishra': 40871,\n",
              " 'worlders': 40872,\n",
              " 'protags': 52135,\n",
              " 'skullcap': 52136,\n",
              " 'dastagir': 57596,\n",
              " 'affairs': 5622,\n",
              " 'wholesome': 7799,\n",
              " 'hymen': 52137,\n",
              " 'paramedics': 25246,\n",
              " 'unpersons': 52138,\n",
              " 'heavyarms': 52139,\n",
              " 'affaire': 52140,\n",
              " 'coulisses': 52141,\n",
              " 'hymer': 40873,\n",
              " 'kremlin': 52142,\n",
              " 'shipments': 30581,\n",
              " 'pixilated': 52143,\n",
              " \"'00s\": 30582,\n",
              " 'diminishing': 18512,\n",
              " 'cinematic': 1357,\n",
              " 'resonates': 14898,\n",
              " 'simplify': 40874,\n",
              " \"nature'\": 40875,\n",
              " 'temptresses': 40876,\n",
              " 'reverence': 16822,\n",
              " 'resonated': 19502,\n",
              " 'dailey': 34722,\n",
              " '2\\x85': 52144,\n",
              " 'treize': 27641,\n",
              " 'majo': 52145,\n",
              " 'kiya': 21910,\n",
              " 'woolnough': 52146,\n",
              " 'thanatos': 39797,\n",
              " 'sandoval': 35731,\n",
              " 'dorama': 40879,\n",
              " \"o'shaughnessy\": 52147,\n",
              " 'tech': 4988,\n",
              " 'fugitives': 32018,\n",
              " 'teck': 30583,\n",
              " \"'e'\": 76125,\n",
              " 'doesn’t': 40881,\n",
              " 'purged': 52149,\n",
              " 'saying': 657,\n",
              " \"martians'\": 41095,\n",
              " 'norliss': 23418,\n",
              " 'dickey': 27642,\n",
              " 'dicker': 52152,\n",
              " \"'sependipity\": 52153,\n",
              " 'padded': 8422,\n",
              " 'ordell': 57792,\n",
              " \"sturges'\": 40882,\n",
              " 'independentcritics': 52154,\n",
              " 'tempted': 5745,\n",
              " \"atkinson's\": 34724,\n",
              " 'hounded': 25247,\n",
              " 'apace': 52155,\n",
              " 'clicked': 15494,\n",
              " \"'humor'\": 30584,\n",
              " \"martino's\": 17177,\n",
              " \"'supporting\": 52156,\n",
              " 'warmongering': 52032,\n",
              " \"zemeckis's\": 34725,\n",
              " 'lube': 21911,\n",
              " 'shocky': 52157,\n",
              " 'plate': 7476,\n",
              " 'plata': 40883,\n",
              " 'sturgess': 40884,\n",
              " \"nerds'\": 40885,\n",
              " 'plato': 20600,\n",
              " 'plath': 34726,\n",
              " 'platt': 40886,\n",
              " 'mcnab': 52159,\n",
              " 'clumsiness': 27643,\n",
              " 'altogether': 3899,\n",
              " 'massacring': 42584,\n",
              " 'bicenntinial': 52160,\n",
              " 'skaal': 40887,\n",
              " 'droning': 14360,\n",
              " 'lds': 8776,\n",
              " 'jaguar': 21912,\n",
              " \"cale's\": 34727,\n",
              " 'nicely': 1777,\n",
              " 'mummy': 4588,\n",
              " \"lot's\": 18513,\n",
              " 'patch': 10086,\n",
              " 'kerkhof': 50202,\n",
              " \"leader's\": 52161,\n",
              " \"'movie\": 27644,\n",
              " 'uncomfirmed': 52162,\n",
              " 'heirloom': 40888,\n",
              " 'wrangle': 47360,\n",
              " 'emotion\\x85': 52163,\n",
              " \"'stargate'\": 52164,\n",
              " 'pinoy': 40889,\n",
              " 'conchatta': 40890,\n",
              " 'broeke': 41128,\n",
              " 'advisedly': 40891,\n",
              " \"barker's\": 17636,\n",
              " 'descours': 52166,\n",
              " 'lots': 772,\n",
              " 'lotr': 9259,\n",
              " 'irs': 9879,\n",
              " 'lott': 52167,\n",
              " 'xvi': 40892,\n",
              " 'irk': 34728,\n",
              " 'irl': 52168,\n",
              " 'ira': 6887,\n",
              " 'belzer': 21913,\n",
              " 'irc': 52169,\n",
              " 'ire': 27645,\n",
              " 'requisites': 40893,\n",
              " 'discipline': 7693,\n",
              " 'lyoko': 52961,\n",
              " 'extend': 11310,\n",
              " 'nature': 873,\n",
              " \"'dickie'\": 52170,\n",
              " 'optimist': 40894,\n",
              " 'lapping': 30586,\n",
              " 'superficial': 3900,\n",
              " 'vestment': 52171,\n",
              " 'extent': 2823,\n",
              " 'tendons': 52172,\n",
              " \"heller's\": 52173,\n",
              " 'quagmires': 52174,\n",
              " 'miyako': 52175,\n",
              " 'moocow': 20601,\n",
              " \"coles'\": 52176,\n",
              " 'lookit': 40895,\n",
              " 'ravenously': 52177,\n",
              " 'levitating': 40896,\n",
              " 'perfunctorily': 52178,\n",
              " 'lookin': 30587,\n",
              " \"lot'\": 40898,\n",
              " 'lookie': 52179,\n",
              " 'fearlessly': 34870,\n",
              " 'libyan': 52181,\n",
              " 'fondles': 40899,\n",
              " 'gopher': 35714,\n",
              " 'wearying': 40901,\n",
              " \"nz's\": 52182,\n",
              " 'minuses': 27646,\n",
              " 'puposelessly': 52183,\n",
              " 'shandling': 52184,\n",
              " 'decapitates': 31268,\n",
              " 'humming': 11929,\n",
              " \"'nother\": 40902,\n",
              " 'smackdown': 21914,\n",
              " 'underdone': 30588,\n",
              " 'frf': 40903,\n",
              " 'triviality': 52185,\n",
              " 'fro': 25248,\n",
              " 'bothers': 8777,\n",
              " \"'kensington\": 52186,\n",
              " 'much': 73,\n",
              " 'muco': 34730,\n",
              " 'wiseguy': 22615,\n",
              " \"richie's\": 27648,\n",
              " 'tonino': 40904,\n",
              " 'unleavened': 52187,\n",
              " 'fry': 11587,\n",
              " \"'tv'\": 40905,\n",
              " 'toning': 40906,\n",
              " 'obese': 14361,\n",
              " 'sensationalized': 30589,\n",
              " 'spiv': 40907,\n",
              " 'spit': 6259,\n",
              " 'arkin': 7364,\n",
              " 'charleton': 21915,\n",
              " 'jeon': 16823,\n",
              " 'boardroom': 21916,\n",
              " 'doubts': 4989,\n",
              " 'spin': 3084,\n",
              " 'hepo': 53083,\n",
              " 'wildcat': 27649,\n",
              " 'venoms': 10584,\n",
              " 'misconstrues': 52191,\n",
              " 'mesmerising': 18514,\n",
              " 'misconstrued': 40908,\n",
              " 'rescinds': 52192,\n",
              " 'prostrate': 52193,\n",
              " 'majid': 40909,\n",
              " 'climbed': 16479,\n",
              " 'canoeing': 34731,\n",
              " 'majin': 52195,\n",
              " 'animie': 57804,\n",
              " 'sylke': 40910,\n",
              " 'conditioned': 14899,\n",
              " 'waddell': 40911,\n",
              " '3\\x85': 52196,\n",
              " 'hyperdrive': 41188,\n",
              " 'conditioner': 34732,\n",
              " 'bricklayer': 53153,\n",
              " 'hong': 2576,\n",
              " 'memoriam': 52198,\n",
              " 'inventively': 30592,\n",
              " \"levant's\": 25249,\n",
              " 'portobello': 20638,\n",
              " 'remand': 52200,\n",
              " 'mummified': 19504,\n",
              " 'honk': 27650,\n",
              " 'spews': 19505,\n",
              " 'visitations': 40912,\n",
              " 'mummifies': 52201,\n",
              " 'cavanaugh': 25250,\n",
              " 'zeon': 23385,\n",
              " \"jungle's\": 40913,\n",
              " 'viertel': 34733,\n",
              " 'frenchmen': 27651,\n",
              " 'torpedoes': 52202,\n",
              " 'schlessinger': 52203,\n",
              " 'torpedoed': 34734,\n",
              " 'blister': 69876,\n",
              " 'cinefest': 52204,\n",
              " 'furlough': 34735,\n",
              " 'mainsequence': 52205,\n",
              " 'mentors': 40914,\n",
              " 'academic': 9094,\n",
              " 'stillness': 20602,\n",
              " 'academia': 40915,\n",
              " 'lonelier': 52206,\n",
              " 'nibby': 52207,\n",
              " \"losers'\": 52208,\n",
              " 'cineastes': 40916,\n",
              " 'corporate': 4449,\n",
              " 'massaging': 40917,\n",
              " 'bellow': 30593,\n",
              " 'absurdities': 19506,\n",
              " 'expetations': 53241,\n",
              " 'nyfiken': 40918,\n",
              " 'mehras': 75638,\n",
              " 'lasse': 52209,\n",
              " 'visability': 52210,\n",
              " 'militarily': 33946,\n",
              " \"elder'\": 52211,\n",
              " 'gainsbourg': 19023,\n",
              " 'hah': 20603,\n",
              " 'hai': 13420,\n",
              " 'haj': 34736,\n",
              " 'hak': 25251,\n",
              " 'hal': 4311,\n",
              " 'ham': 4892,\n",
              " 'duffer': 53259,\n",
              " 'haa': 52213,\n",
              " 'had': 66,\n",
              " 'advancement': 11930,\n",
              " 'hag': 16825,\n",
              " \"hand'\": 25252,\n",
              " 'hay': 13421,\n",
              " 'mcnamara': 20604,\n",
              " \"mozart's\": 52214,\n",
              " 'duffel': 30731,\n",
              " 'haq': 30594,\n",
              " 'har': 13887,\n",
              " 'has': 44,\n",
              " 'hat': 2401,\n",
              " 'hav': 40919,\n",
              " 'haw': 30595,\n",
              " 'figtings': 52215,\n",
              " 'elders': 15495,\n",
              " 'underpanted': 52216,\n",
              " 'pninson': 52217,\n",
              " 'unequivocally': 27652,\n",
              " \"barbara's\": 23673,\n",
              " \"bello'\": 52219,\n",
              " 'indicative': 12997,\n",
              " 'yawnfest': 40920,\n",
              " 'hexploitation': 52220,\n",
              " \"loder's\": 52221,\n",
              " 'sleuthing': 27653,\n",
              " \"justin's\": 32622,\n",
              " \"'ball\": 52222,\n",
              " \"'summer\": 52223,\n",
              " \"'demons'\": 34935,\n",
              " \"mormon's\": 52225,\n",
              " \"laughton's\": 34737,\n",
              " 'debell': 52226,\n",
              " 'shipyard': 39724,\n",
              " 'unabashedly': 30597,\n",
              " 'disks': 40401,\n",
              " 'crowd': 2290,\n",
              " 'crowe': 10087,\n",
              " \"vancouver's\": 56434,\n",
              " 'mosques': 34738,\n",
              " 'crown': 6627,\n",
              " 'culpas': 52227,\n",
              " 'crows': 27654,\n",
              " 'surrell': 53344,\n",
              " 'flowless': 52229,\n",
              " 'sheirk': 52230,\n",
              " \"'three\": 40923,\n",
              " \"peterson'\": 52231,\n",
              " 'ooverall': 52232,\n",
              " 'perchance': 40924,\n",
              " 'bottom': 1321,\n",
              " 'chabert': 53363,\n",
              " 'sneha': 52233,\n",
              " 'inhuman': 13888,\n",
              " 'ichii': 52234,\n",
              " 'ursla': 52235,\n",
              " 'completly': 30598,\n",
              " 'moviedom': 40925,\n",
              " 'raddick': 52236,\n",
              " 'brundage': 51995,\n",
              " 'brigades': 40926,\n",
              " 'starring': 1181,\n",
              " \"'goal'\": 52237,\n",
              " 'caskets': 52238,\n",
              " 'willcock': 52239,\n",
              " \"threesome's\": 52240,\n",
              " \"mosque'\": 52241,\n",
              " \"cover's\": 52242,\n",
              " 'spaceships': 17637,\n",
              " 'anomalous': 40927,\n",
              " 'ptsd': 27655,\n",
              " 'shirdan': 52243,\n",
              " 'obscenity': 21962,\n",
              " 'lemmings': 30599,\n",
              " 'duccio': 30600,\n",
              " \"levene's\": 52244,\n",
              " \"'gorby'\": 52245,\n",
              " \"teenager's\": 25255,\n",
              " 'marshall': 5340,\n",
              " 'honeymoon': 9095,\n",
              " 'shoots': 3231,\n",
              " 'despised': 12258,\n",
              " 'okabasho': 52246,\n",
              " 'fabric': 8289,\n",
              " 'cannavale': 18515,\n",
              " 'raped': 3537,\n",
              " \"tutt's\": 52247,\n",
              " 'grasping': 17638,\n",
              " 'despises': 18516,\n",
              " \"thief's\": 40928,\n",
              " 'rapes': 8926,\n",
              " 'raper': 52248,\n",
              " \"eyre'\": 27656,\n",
              " 'walchek': 52249,\n",
              " \"elmo's\": 23386,\n",
              " 'perfumes': 40929,\n",
              " 'spurting': 21918,\n",
              " \"exposition'\\x85\": 52250,\n",
              " 'denoting': 52251,\n",
              " 'thesaurus': 34740,\n",
              " \"shoot'\": 40930,\n",
              " 'bonejack': 49759,\n",
              " 'simpsonian': 52253,\n",
              " 'hebetude': 30601,\n",
              " \"hallow's\": 34741,\n",
              " 'desperation\\x85': 52254,\n",
              " 'incinerator': 34742,\n",
              " 'congratulations': 10308,\n",
              " 'humbled': 52255,\n",
              " \"else's\": 5924,\n",
              " 'trelkovski': 40845,\n",
              " \"rape'\": 52256,\n",
              " \"'chapters'\": 59386,\n",
              " '1600s': 52257,\n",
              " 'martian': 7253,\n",
              " 'nicest': 25256,\n",
              " 'eyred': 52259,\n",
              " 'passenger': 9457,\n",
              " 'disgrace': 6041,\n",
              " 'moderne': 52260,\n",
              " 'barrymore': 5120,\n",
              " 'yankovich': 52261,\n",
              " 'moderns': 40931,\n",
              " 'studliest': 52262,\n",
              " 'bedsheet': 52263,\n",
              " 'decapitation': 14900,\n",
              " 'slurring': 52264,\n",
              " \"'nunsploitation'\": 52265,\n",
              " \"'character'\": 34743,\n",
              " 'cambodia': 9880,\n",
              " 'rebelious': 52266,\n",
              " 'pasadena': 27657,\n",
              " 'crowne': 40932,\n",
              " \"'bedchamber\": 52267,\n",
              " 'conjectural': 52268,\n",
              " 'appologize': 52269,\n",
              " 'halfassing': 52270,\n",
              " 'paycheque': 57816,\n",
              " 'palms': 20606,\n",
              " \"'islands\": 52271,\n",
              " 'hawked': 40933,\n",
              " 'palme': 21919,\n",
              " 'conservatively': 40934,\n",
              " 'larp': 64007,\n",
              " 'palma': 5558,\n",
              " 'smelling': 21920,\n",
              " 'aragorn': 12998,\n",
              " 'hawker': 52272,\n",
              " 'hawkes': 52273,\n",
              " 'explosions': 3975,\n",
              " 'loren': 8059,\n",
              " \"pyle's\": 52274,\n",
              " 'shootout': 6704,\n",
              " \"mike's\": 18517,\n",
              " \"driscoll's\": 52275,\n",
              " 'cogsworth': 40935,\n",
              " \"britian's\": 52276,\n",
              " 'childs': 34744,\n",
              " \"portrait's\": 52277,\n",
              " 'chain': 3626,\n",
              " 'whoever': 2497,\n",
              " 'puttered': 52278,\n",
              " 'childe': 52279,\n",
              " 'maywether': 52280,\n",
              " 'chair': 3036,\n",
              " \"rance's\": 52281,\n",
              " 'machu': 34745,\n",
              " 'ballet': 4517,\n",
              " 'grapples': 34746,\n",
              " 'summerize': 76152,\n",
              " 'freelance': 30603,\n",
              " \"andrea's\": 52283,\n",
              " '\\x91very': 52284,\n",
              " 'coolidge': 45879,\n",
              " 'mache': 18518,\n",
              " 'balled': 52285,\n",
              " 'grappled': 40937,\n",
              " 'macha': 18519,\n",
              " 'underlining': 21921,\n",
              " 'macho': 5623,\n",
              " 'oversight': 19507,\n",
              " 'machi': 25257,\n",
              " 'verbally': 11311,\n",
              " 'tenacious': 21922,\n",
              " 'windshields': 40938,\n",
              " 'paychecks': 18557,\n",
              " 'jerk': 3396,\n",
              " \"good'\": 11931,\n",
              " 'prancer': 34748,\n",
              " 'prances': 21923,\n",
              " 'olympus': 52286,\n",
              " 'lark': 21924,\n",
              " 'embark': 10785,\n",
              " 'gloomy': 7365,\n",
              " 'jehaan': 52287,\n",
              " 'turaqui': 52288,\n",
              " \"child'\": 20607,\n",
              " 'locked': 2894,\n",
              " 'pranced': 52289,\n",
              " 'exact': 2588,\n",
              " 'unattuned': 52290,\n",
              " 'minute': 783,\n",
              " 'skewed': 16118,\n",
              " 'hodgins': 40940,\n",
              " 'skewer': 34749,\n",
              " 'think\\x85': 52291,\n",
              " 'rosenstein': 38765,\n",
              " 'helmit': 52292,\n",
              " 'wrestlemanias': 34750,\n",
              " 'hindered': 16826,\n",
              " \"martha's\": 30604,\n",
              " 'cheree': 52293,\n",
              " \"pluckin'\": 52294,\n",
              " 'ogles': 40941,\n",
              " 'heavyweight': 11932,\n",
              " 'aada': 82190,\n",
              " 'chopping': 11312,\n",
              " 'strongboy': 61534,\n",
              " 'hegemonic': 41342,\n",
              " 'adorns': 40942,\n",
              " 'xxth': 41346,\n",
              " 'nobuhiro': 34751,\n",
              " 'capitães': 52298,\n",
              " 'kavogianni': 52299,\n",
              " 'antwerp': 13422,\n",
              " 'celebrated': 6538,\n",
              " 'roarke': 52300,\n",
              " 'baggins': 40943,\n",
              " 'cheeseburgers': 31270,\n",
              " 'matras': 52301,\n",
              " \"nineties'\": 52302,\n",
              " \"'craig'\": 52303,\n",
              " 'celebrates': 12999,\n",
              " 'unintentionally': 3383,\n",
              " 'drafted': 14362,\n",
              " 'climby': 52304,\n",
              " '303': 52305,\n",
              " 'oldies': 18520,\n",
              " 'climbs': 9096,\n",
              " 'honour': 9655,\n",
              " 'plucking': 34752,\n",
              " '305': 30074,\n",
              " 'address': 5514,\n",
              " 'menjou': 40944,\n",
              " \"'freak'\": 42592,\n",
              " 'dwindling': 19508,\n",
              " 'benson': 9458,\n",
              " 'white’s': 52307,\n",
              " 'shamelessness': 40945,\n",
              " 'impacted': 21925,\n",
              " 'upatz': 52308,\n",
              " 'cusack': 3840,\n",
              " \"flavia's\": 37567,\n",
              " 'effette': 52309,\n",
              " 'influx': 34753,\n",
              " 'boooooooo': 52310,\n",
              " 'dimitrova': 52311,\n",
              " 'houseman': 13423,\n",
              " 'bigas': 25259,\n",
              " 'boylen': 52312,\n",
              " 'phillipenes': 52313,\n",
              " 'fakery': 40946,\n",
              " \"grandpa's\": 27658,\n",
              " 'darnell': 27659,\n",
              " 'undergone': 19509,\n",
              " 'handbags': 52315,\n",
              " 'perished': 21926,\n",
              " 'pooped': 37778,\n",
              " 'vigour': 27660,\n",
              " 'opposed': 3627,\n",
              " 'etude': 52316,\n",
              " \"caine's\": 11799,\n",
              " 'doozers': 52317,\n",
              " 'photojournals': 34754,\n",
              " 'perishes': 52318,\n",
              " 'constrains': 34755,\n",
              " 'migenes': 40948,\n",
              " 'consoled': 30605,\n",
              " 'alastair': 16827,\n",
              " 'wvs': 52319,\n",
              " 'ooooooh': 52320,\n",
              " 'approving': 34756,\n",
              " 'consoles': 40949,\n",
              " 'disparagement': 52064,\n",
              " 'futureistic': 52322,\n",
              " 'rebounding': 52323,\n",
              " \"'date\": 52324,\n",
              " 'gregoire': 52325,\n",
              " 'rutherford': 21927,\n",
              " 'americanised': 34757,\n",
              " 'novikov': 82196,\n",
              " 'following': 1042,\n",
              " 'munroe': 34758,\n",
              " \"morita'\": 52326,\n",
              " 'christenssen': 52327,\n",
              " 'oatmeal': 23106,\n",
              " 'fossey': 25260,\n",
              " 'livered': 40950,\n",
              " 'listens': 13000,\n",
              " \"'marci\": 76164,\n",
              " \"otis's\": 52330,\n",
              " 'thanking': 23387,\n",
              " 'maude': 16019,\n",
              " 'extensions': 34759,\n",
              " 'ameteurish': 52332,\n",
              " \"commender's\": 52333,\n",
              " 'agricultural': 27661,\n",
              " 'convincingly': 4518,\n",
              " 'fueled': 17639,\n",
              " 'mahattan': 54014,\n",
              " \"paris's\": 40952,\n",
              " 'vulkan': 52336,\n",
              " 'stapes': 52337,\n",
              " 'odysessy': 52338,\n",
              " 'harmon': 12259,\n",
              " 'surfing': 4252,\n",
              " 'halloran': 23494,\n",
              " 'unbelieveably': 49580,\n",
              " \"'offed'\": 52339,\n",
              " 'quadrant': 30607,\n",
              " 'inhabiting': 19510,\n",
              " 'nebbish': 34760,\n",
              " 'forebears': 40953,\n",
              " 'skirmish': 34761,\n",
              " 'ocassionally': 52340,\n",
              " \"'resist\": 52341,\n",
              " 'impactful': 21928,\n",
              " 'spicier': 52342,\n",
              " 'touristy': 40954,\n",
              " \"'football'\": 52343,\n",
              " 'webpage': 40955,\n",
              " 'exurbia': 52345,\n",
              " 'jucier': 52346,\n",
              " 'professors': 14901,\n",
              " 'structuring': 34762,\n",
              " 'jig': 30608,\n",
              " 'overlord': 40956,\n",
              " 'disconnect': 25261,\n",
              " 'sniffle': 82201,\n",
              " 'slimeball': 40957,\n",
              " 'jia': 40958,\n",
              " 'milked': 16828,\n",
              " 'banjoes': 40959,\n",
              " 'jim': 1237,\n",
              " 'workforces': 52348,\n",
              " 'jip': 52349,\n",
              " 'rotweiller': 52350,\n",
              " 'mundaneness': 34763,\n",
              " \"'ninja'\": 52351,\n",
              " \"dead'\": 11040,\n",
              " \"cipriani's\": 40960,\n",
              " 'modestly': 20608,\n",
              " \"professor'\": 52352,\n",
              " 'shacked': 40961,\n",
              " 'bashful': 34764,\n",
              " 'sorter': 23388,\n",
              " 'overpowering': 16120,\n",
              " 'workmanlike': 18521,\n",
              " 'henpecked': 27662,\n",
              " 'sorted': 18522,\n",
              " \"jōb's\": 52354,\n",
              " \"'always\": 52355,\n",
              " \"'baptists\": 34765,\n",
              " 'dreamcatchers': 52356,\n",
              " \"'silence'\": 52357,\n",
              " 'hickory': 21929,\n",
              " 'fun\\x97yet': 52358,\n",
              " 'breakumentary': 52359,\n",
              " 'didn': 15496,\n",
              " 'didi': 52360,\n",
              " 'pealing': 52361,\n",
              " 'dispite': 40962,\n",
              " \"italy's\": 25262,\n",
              " 'instability': 21930,\n",
              " 'quarter': 6539,\n",
              " 'quartet': 12608,\n",
              " 'padmé': 52362,\n",
              " \"'bleedmedry\": 52363,\n",
              " 'pahalniuk': 52364,\n",
              " 'honduras': 52365,\n",
              " 'bursting': 10786,\n",
              " \"pablo's\": 41465,\n",
              " 'irremediably': 52367,\n",
              " 'presages': 40963,\n",
              " 'bowlegged': 57832,\n",
              " 'dalip': 65183,\n",
              " 'entering': 6260,\n",
              " 'newsradio': 76172,\n",
              " 'presaged': 54150,\n",
              " \"giallo's\": 27663,\n",
              " 'bouyant': 40964,\n",
              " 'amerterish': 52368,\n",
              " 'rajni': 18523,\n",
              " 'leeves': 30610,\n",
              " 'macauley': 34767,\n",
              " 'seriously': 612,\n",
              " 'sugercoma': 52369,\n",
              " 'grimstead': 52370,\n",
              " \"'fairy'\": 52371,\n",
              " 'zenda': 30611,\n",
              " \"'twins'\": 52372,\n",
              " 'realisation': 17640,\n",
              " 'highsmith': 27664,\n",
              " 'raunchy': 7817,\n",
              " 'incentives': 40965,\n",
              " 'flatson': 52374,\n",
              " 'snooker': 35097,\n",
              " 'crazies': 16829,\n",
              " 'crazier': 14902,\n",
              " 'grandma': 7094,\n",
              " 'napunsaktha': 52375,\n",
              " 'workmanship': 30612,\n",
              " 'reisner': 52376,\n",
              " \"sanford's\": 61306,\n",
              " '\\x91doña': 52377,\n",
              " 'modest': 6108,\n",
              " \"everything's\": 19153,\n",
              " 'hamer': 40966,\n",
              " \"couldn't'\": 52379,\n",
              " 'quibble': 13001,\n",
              " 'socking': 52380,\n",
              " 'tingler': 21931,\n",
              " 'gutman': 52381,\n",
              " 'lachlan': 40967,\n",
              " 'tableaus': 52382,\n",
              " 'headbanger': 52383,\n",
              " 'spoken': 2847,\n",
              " 'cerebrally': 34768,\n",
              " \"'road\": 23490,\n",
              " 'tableaux': 21932,\n",
              " \"proust's\": 40968,\n",
              " 'periodical': 40969,\n",
              " \"shoveller's\": 52385,\n",
              " 'tamara': 25263,\n",
              " 'affords': 17641,\n",
              " 'concert': 3249,\n",
              " \"yara's\": 87955,\n",
              " 'someome': 52386,\n",
              " 'lingering': 8424,\n",
              " \"abraham's\": 41511,\n",
              " 'beesley': 34769,\n",
              " 'cherbourg': 34770,\n",
              " 'kagan': 28624,\n",
              " 'snatch': 9097,\n",
              " \"miyazaki's\": 9260,\n",
              " 'absorbs': 25264,\n",
              " \"koltai's\": 40970,\n",
              " 'tingled': 64027,\n",
              " 'crossroads': 19511,\n",
              " 'rehab': 16121,\n",
              " 'falworth': 52389,\n",
              " 'sequals': 52390,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYiOXdB88E_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311fddad-a968-4273-daf6-ff358b7458c0"
      },
      "source": [
        "reverse_word_index\n",
        "#The list is reversed."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{34701: 'fawn',\n",
              " 52006: 'tsukino',\n",
              " 52007: 'nunnery',\n",
              " 16816: 'sonja',\n",
              " 63951: 'vani',\n",
              " 1408: 'woods',\n",
              " 16115: 'spiders',\n",
              " 2345: 'hanging',\n",
              " 2289: 'woody',\n",
              " 52008: 'trawling',\n",
              " 52009: \"hold's\",\n",
              " 11307: 'comically',\n",
              " 40830: 'localized',\n",
              " 30568: 'disobeying',\n",
              " 52010: \"'royale\",\n",
              " 40831: \"harpo's\",\n",
              " 52011: 'canet',\n",
              " 19313: 'aileen',\n",
              " 52012: 'acurately',\n",
              " 52013: \"diplomat's\",\n",
              " 25242: 'rickman',\n",
              " 6746: 'arranged',\n",
              " 52014: 'rumbustious',\n",
              " 52015: 'familiarness',\n",
              " 52016: \"spider'\",\n",
              " 68804: 'hahahah',\n",
              " 52017: \"wood'\",\n",
              " 40833: 'transvestism',\n",
              " 34702: \"hangin'\",\n",
              " 2338: 'bringing',\n",
              " 40834: 'seamier',\n",
              " 34703: 'wooded',\n",
              " 52018: 'bravora',\n",
              " 16817: 'grueling',\n",
              " 1636: 'wooden',\n",
              " 16818: 'wednesday',\n",
              " 52019: \"'prix\",\n",
              " 34704: 'altagracia',\n",
              " 52020: 'circuitry',\n",
              " 11585: 'crotch',\n",
              " 57766: 'busybody',\n",
              " 52021: \"tart'n'tangy\",\n",
              " 14129: 'burgade',\n",
              " 52023: 'thrace',\n",
              " 11038: \"tom's\",\n",
              " 52025: 'snuggles',\n",
              " 29114: 'francesco',\n",
              " 52027: 'complainers',\n",
              " 52125: 'templarios',\n",
              " 40835: '272',\n",
              " 52028: '273',\n",
              " 52130: 'zaniacs',\n",
              " 34706: '275',\n",
              " 27631: 'consenting',\n",
              " 40836: 'snuggled',\n",
              " 15492: 'inanimate',\n",
              " 52030: 'uality',\n",
              " 11926: 'bronte',\n",
              " 4010: 'errors',\n",
              " 3230: 'dialogs',\n",
              " 52031: \"yomada's\",\n",
              " 34707: \"madman's\",\n",
              " 30585: 'dialoge',\n",
              " 52033: 'usenet',\n",
              " 40837: 'videodrome',\n",
              " 26338: \"kid'\",\n",
              " 52034: 'pawed',\n",
              " 30569: \"'girlfriend'\",\n",
              " 52035: \"'pleasure\",\n",
              " 52036: \"'reloaded'\",\n",
              " 40839: \"kazakos'\",\n",
              " 52037: 'rocque',\n",
              " 52038: 'mailings',\n",
              " 11927: 'brainwashed',\n",
              " 16819: 'mcanally',\n",
              " 52039: \"tom''\",\n",
              " 25243: 'kurupt',\n",
              " 21905: 'affiliated',\n",
              " 52040: 'babaganoosh',\n",
              " 40840: \"noe's\",\n",
              " 40841: 'quart',\n",
              " 359: 'kids',\n",
              " 5034: 'uplifting',\n",
              " 7093: 'controversy',\n",
              " 21906: 'kida',\n",
              " 23379: 'kidd',\n",
              " 52041: \"error'\",\n",
              " 52042: 'neurologist',\n",
              " 18510: 'spotty',\n",
              " 30570: 'cobblers',\n",
              " 9878: 'projection',\n",
              " 40842: 'fastforwarding',\n",
              " 52043: 'sters',\n",
              " 52044: \"eggar's\",\n",
              " 52045: 'etherything',\n",
              " 40843: 'gateshead',\n",
              " 34708: 'airball',\n",
              " 25244: 'unsinkable',\n",
              " 7180: 'stern',\n",
              " 52046: \"cervi's\",\n",
              " 40844: 'dnd',\n",
              " 11586: 'dna',\n",
              " 20598: 'insecurity',\n",
              " 52047: \"'reboot'\",\n",
              " 11037: 'trelkovsky',\n",
              " 52048: 'jaekel',\n",
              " 52049: 'sidebars',\n",
              " 52050: \"sforza's\",\n",
              " 17633: 'distortions',\n",
              " 52051: 'mutinies',\n",
              " 30602: 'sermons',\n",
              " 40846: '7ft',\n",
              " 52052: 'boobage',\n",
              " 52053: \"o'bannon's\",\n",
              " 23380: 'populations',\n",
              " 52054: 'chulak',\n",
              " 27633: 'mesmerize',\n",
              " 52055: 'quinnell',\n",
              " 10307: 'yahoo',\n",
              " 52057: 'meteorologist',\n",
              " 42577: 'beswick',\n",
              " 15493: 'boorman',\n",
              " 40847: 'voicework',\n",
              " 52058: \"ster'\",\n",
              " 22922: 'blustering',\n",
              " 52059: 'hj',\n",
              " 27634: 'intake',\n",
              " 5621: 'morally',\n",
              " 40849: 'jumbling',\n",
              " 52060: 'bowersock',\n",
              " 52061: \"'porky's'\",\n",
              " 16821: 'gershon',\n",
              " 40850: 'ludicrosity',\n",
              " 52062: 'coprophilia',\n",
              " 40851: 'expressively',\n",
              " 19500: \"india's\",\n",
              " 34710: \"post's\",\n",
              " 52063: 'wana',\n",
              " 5283: 'wang',\n",
              " 30571: 'wand',\n",
              " 25245: 'wane',\n",
              " 52321: 'edgeways',\n",
              " 34711: 'titanium',\n",
              " 40852: 'pinta',\n",
              " 178: 'want',\n",
              " 30572: 'pinto',\n",
              " 52065: 'whoopdedoodles',\n",
              " 21908: 'tchaikovsky',\n",
              " 2103: 'travel',\n",
              " 52066: \"'victory'\",\n",
              " 11928: 'copious',\n",
              " 22433: 'gouge',\n",
              " 52067: \"chapters'\",\n",
              " 6702: 'barbra',\n",
              " 30573: 'uselessness',\n",
              " 52068: \"wan'\",\n",
              " 27635: 'assimilated',\n",
              " 16116: 'petiot',\n",
              " 52069: 'most\\x85and',\n",
              " 3930: 'dinosaurs',\n",
              " 352: 'wrong',\n",
              " 52070: 'seda',\n",
              " 52071: 'stollen',\n",
              " 34712: 'sentencing',\n",
              " 40853: 'ouroboros',\n",
              " 40854: 'assimilates',\n",
              " 40855: 'colorfully',\n",
              " 27636: 'glenne',\n",
              " 52072: 'dongen',\n",
              " 4760: 'subplots',\n",
              " 52073: 'kiloton',\n",
              " 23381: 'chandon',\n",
              " 34713: \"effect'\",\n",
              " 27637: 'snugly',\n",
              " 40856: 'kuei',\n",
              " 9092: 'welcomed',\n",
              " 30071: 'dishonor',\n",
              " 52075: 'concurrence',\n",
              " 23382: 'stoicism',\n",
              " 14896: \"guys'\",\n",
              " 52077: \"beroemd'\",\n",
              " 6703: 'butcher',\n",
              " 40857: \"melfi's\",\n",
              " 30623: 'aargh',\n",
              " 20599: 'playhouse',\n",
              " 11308: 'wickedly',\n",
              " 1180: 'fit',\n",
              " 52078: 'labratory',\n",
              " 40859: 'lifeline',\n",
              " 1927: 'screaming',\n",
              " 4287: 'fix',\n",
              " 52079: 'cineliterate',\n",
              " 52080: 'fic',\n",
              " 52081: 'fia',\n",
              " 34714: 'fig',\n",
              " 52082: 'fmvs',\n",
              " 52083: 'fie',\n",
              " 52084: 'reentered',\n",
              " 30574: 'fin',\n",
              " 52085: 'doctresses',\n",
              " 52086: 'fil',\n",
              " 12606: 'zucker',\n",
              " 31931: 'ached',\n",
              " 52088: 'counsil',\n",
              " 52089: 'paterfamilias',\n",
              " 13885: 'songwriter',\n",
              " 34715: 'shivam',\n",
              " 9654: 'hurting',\n",
              " 299: 'effects',\n",
              " 52090: 'slauther',\n",
              " 52091: \"'flame'\",\n",
              " 52092: 'sommerset',\n",
              " 52093: 'interwhined',\n",
              " 27638: 'whacking',\n",
              " 52094: 'bartok',\n",
              " 8775: 'barton',\n",
              " 21909: 'frewer',\n",
              " 52095: \"fi'\",\n",
              " 6192: 'ingrid',\n",
              " 30575: 'stribor',\n",
              " 52096: 'approporiately',\n",
              " 52097: 'wobblyhand',\n",
              " 52098: 'tantalisingly',\n",
              " 52099: 'ankylosaurus',\n",
              " 17634: 'parasites',\n",
              " 52100: 'childen',\n",
              " 52101: \"jenkins'\",\n",
              " 52102: 'metafiction',\n",
              " 17635: 'golem',\n",
              " 40860: 'indiscretion',\n",
              " 23383: \"reeves'\",\n",
              " 57781: \"inamorata's\",\n",
              " 52104: 'brittannica',\n",
              " 7916: 'adapt',\n",
              " 30576: \"russo's\",\n",
              " 48246: 'guitarists',\n",
              " 10553: 'abbott',\n",
              " 40861: 'abbots',\n",
              " 17649: 'lanisha',\n",
              " 40863: 'magickal',\n",
              " 52105: 'mattter',\n",
              " 52106: \"'willy\",\n",
              " 34716: 'pumpkins',\n",
              " 52107: 'stuntpeople',\n",
              " 30577: 'estimate',\n",
              " 40864: 'ugghhh',\n",
              " 11309: 'gameplay',\n",
              " 52108: \"wern't\",\n",
              " 40865: \"n'sync\",\n",
              " 16117: 'sickeningly',\n",
              " 40866: 'chiara',\n",
              " 4011: 'disturbed',\n",
              " 40867: 'portmanteau',\n",
              " 52109: 'ineffectively',\n",
              " 82143: \"duchonvey's\",\n",
              " 37519: \"nasty'\",\n",
              " 1285: 'purpose',\n",
              " 52112: 'lazers',\n",
              " 28105: 'lightened',\n",
              " 52113: 'kaliganj',\n",
              " 52114: 'popularism',\n",
              " 18511: \"damme's\",\n",
              " 30578: 'stylistics',\n",
              " 52115: 'mindgaming',\n",
              " 46449: 'spoilerish',\n",
              " 52117: \"'corny'\",\n",
              " 34718: 'boerner',\n",
              " 6792: 'olds',\n",
              " 52118: 'bakelite',\n",
              " 27639: 'renovated',\n",
              " 27640: 'forrester',\n",
              " 52119: \"lumiere's\",\n",
              " 52024: 'gaskets',\n",
              " 884: 'needed',\n",
              " 34719: 'smight',\n",
              " 1297: 'master',\n",
              " 25905: \"edie's\",\n",
              " 40868: 'seeber',\n",
              " 52120: 'hiya',\n",
              " 52121: 'fuzziness',\n",
              " 14897: 'genesis',\n",
              " 12607: 'rewards',\n",
              " 30579: 'enthrall',\n",
              " 40869: \"'about\",\n",
              " 52122: \"recollection's\",\n",
              " 11039: 'mutilated',\n",
              " 52123: 'fatherlands',\n",
              " 52124: \"fischer's\",\n",
              " 5399: 'positively',\n",
              " 34705: '270',\n",
              " 34720: 'ahmed',\n",
              " 9836: 'zatoichi',\n",
              " 13886: 'bannister',\n",
              " 52127: 'anniversaries',\n",
              " 30580: \"helm's\",\n",
              " 52128: \"'work'\",\n",
              " 34721: 'exclaimed',\n",
              " 52129: \"'unfunny'\",\n",
              " 52029: '274',\n",
              " 544: 'feeling',\n",
              " 52131: \"wanda's\",\n",
              " 33266: 'dolan',\n",
              " 52133: '278',\n",
              " 52134: 'peacoat',\n",
              " 40870: 'brawny',\n",
              " 40871: 'mishra',\n",
              " 40872: 'worlders',\n",
              " 52135: 'protags',\n",
              " 52136: 'skullcap',\n",
              " 57596: 'dastagir',\n",
              " 5622: 'affairs',\n",
              " 7799: 'wholesome',\n",
              " 52137: 'hymen',\n",
              " 25246: 'paramedics',\n",
              " 52138: 'unpersons',\n",
              " 52139: 'heavyarms',\n",
              " 52140: 'affaire',\n",
              " 52141: 'coulisses',\n",
              " 40873: 'hymer',\n",
              " 52142: 'kremlin',\n",
              " 30581: 'shipments',\n",
              " 52143: 'pixilated',\n",
              " 30582: \"'00s\",\n",
              " 18512: 'diminishing',\n",
              " 1357: 'cinematic',\n",
              " 14898: 'resonates',\n",
              " 40874: 'simplify',\n",
              " 40875: \"nature'\",\n",
              " 40876: 'temptresses',\n",
              " 16822: 'reverence',\n",
              " 19502: 'resonated',\n",
              " 34722: 'dailey',\n",
              " 52144: '2\\x85',\n",
              " 27641: 'treize',\n",
              " 52145: 'majo',\n",
              " 21910: 'kiya',\n",
              " 52146: 'woolnough',\n",
              " 39797: 'thanatos',\n",
              " 35731: 'sandoval',\n",
              " 40879: 'dorama',\n",
              " 52147: \"o'shaughnessy\",\n",
              " 4988: 'tech',\n",
              " 32018: 'fugitives',\n",
              " 30583: 'teck',\n",
              " 76125: \"'e'\",\n",
              " 40881: 'doesn’t',\n",
              " 52149: 'purged',\n",
              " 657: 'saying',\n",
              " 41095: \"martians'\",\n",
              " 23418: 'norliss',\n",
              " 27642: 'dickey',\n",
              " 52152: 'dicker',\n",
              " 52153: \"'sependipity\",\n",
              " 8422: 'padded',\n",
              " 57792: 'ordell',\n",
              " 40882: \"sturges'\",\n",
              " 52154: 'independentcritics',\n",
              " 5745: 'tempted',\n",
              " 34724: \"atkinson's\",\n",
              " 25247: 'hounded',\n",
              " 52155: 'apace',\n",
              " 15494: 'clicked',\n",
              " 30584: \"'humor'\",\n",
              " 17177: \"martino's\",\n",
              " 52156: \"'supporting\",\n",
              " 52032: 'warmongering',\n",
              " 34725: \"zemeckis's\",\n",
              " 21911: 'lube',\n",
              " 52157: 'shocky',\n",
              " 7476: 'plate',\n",
              " 40883: 'plata',\n",
              " 40884: 'sturgess',\n",
              " 40885: \"nerds'\",\n",
              " 20600: 'plato',\n",
              " 34726: 'plath',\n",
              " 40886: 'platt',\n",
              " 52159: 'mcnab',\n",
              " 27643: 'clumsiness',\n",
              " 3899: 'altogether',\n",
              " 42584: 'massacring',\n",
              " 52160: 'bicenntinial',\n",
              " 40887: 'skaal',\n",
              " 14360: 'droning',\n",
              " 8776: 'lds',\n",
              " 21912: 'jaguar',\n",
              " 34727: \"cale's\",\n",
              " 1777: 'nicely',\n",
              " 4588: 'mummy',\n",
              " 18513: \"lot's\",\n",
              " 10086: 'patch',\n",
              " 50202: 'kerkhof',\n",
              " 52161: \"leader's\",\n",
              " 27644: \"'movie\",\n",
              " 52162: 'uncomfirmed',\n",
              " 40888: 'heirloom',\n",
              " 47360: 'wrangle',\n",
              " 52163: 'emotion\\x85',\n",
              " 52164: \"'stargate'\",\n",
              " 40889: 'pinoy',\n",
              " 40890: 'conchatta',\n",
              " 41128: 'broeke',\n",
              " 40891: 'advisedly',\n",
              " 17636: \"barker's\",\n",
              " 52166: 'descours',\n",
              " 772: 'lots',\n",
              " 9259: 'lotr',\n",
              " 9879: 'irs',\n",
              " 52167: 'lott',\n",
              " 40892: 'xvi',\n",
              " 34728: 'irk',\n",
              " 52168: 'irl',\n",
              " 6887: 'ira',\n",
              " 21913: 'belzer',\n",
              " 52169: 'irc',\n",
              " 27645: 'ire',\n",
              " 40893: 'requisites',\n",
              " 7693: 'discipline',\n",
              " 52961: 'lyoko',\n",
              " 11310: 'extend',\n",
              " 873: 'nature',\n",
              " 52170: \"'dickie'\",\n",
              " 40894: 'optimist',\n",
              " 30586: 'lapping',\n",
              " 3900: 'superficial',\n",
              " 52171: 'vestment',\n",
              " 2823: 'extent',\n",
              " 52172: 'tendons',\n",
              " 52173: \"heller's\",\n",
              " 52174: 'quagmires',\n",
              " 52175: 'miyako',\n",
              " 20601: 'moocow',\n",
              " 52176: \"coles'\",\n",
              " 40895: 'lookit',\n",
              " 52177: 'ravenously',\n",
              " 40896: 'levitating',\n",
              " 52178: 'perfunctorily',\n",
              " 30587: 'lookin',\n",
              " 40898: \"lot'\",\n",
              " 52179: 'lookie',\n",
              " 34870: 'fearlessly',\n",
              " 52181: 'libyan',\n",
              " 40899: 'fondles',\n",
              " 35714: 'gopher',\n",
              " 40901: 'wearying',\n",
              " 52182: \"nz's\",\n",
              " 27646: 'minuses',\n",
              " 52183: 'puposelessly',\n",
              " 52184: 'shandling',\n",
              " 31268: 'decapitates',\n",
              " 11929: 'humming',\n",
              " 40902: \"'nother\",\n",
              " 21914: 'smackdown',\n",
              " 30588: 'underdone',\n",
              " 40903: 'frf',\n",
              " 52185: 'triviality',\n",
              " 25248: 'fro',\n",
              " 8777: 'bothers',\n",
              " 52186: \"'kensington\",\n",
              " 73: 'much',\n",
              " 34730: 'muco',\n",
              " 22615: 'wiseguy',\n",
              " 27648: \"richie's\",\n",
              " 40904: 'tonino',\n",
              " 52187: 'unleavened',\n",
              " 11587: 'fry',\n",
              " 40905: \"'tv'\",\n",
              " 40906: 'toning',\n",
              " 14361: 'obese',\n",
              " 30589: 'sensationalized',\n",
              " 40907: 'spiv',\n",
              " 6259: 'spit',\n",
              " 7364: 'arkin',\n",
              " 21915: 'charleton',\n",
              " 16823: 'jeon',\n",
              " 21916: 'boardroom',\n",
              " 4989: 'doubts',\n",
              " 3084: 'spin',\n",
              " 53083: 'hepo',\n",
              " 27649: 'wildcat',\n",
              " 10584: 'venoms',\n",
              " 52191: 'misconstrues',\n",
              " 18514: 'mesmerising',\n",
              " 40908: 'misconstrued',\n",
              " 52192: 'rescinds',\n",
              " 52193: 'prostrate',\n",
              " 40909: 'majid',\n",
              " 16479: 'climbed',\n",
              " 34731: 'canoeing',\n",
              " 52195: 'majin',\n",
              " 57804: 'animie',\n",
              " 40910: 'sylke',\n",
              " 14899: 'conditioned',\n",
              " 40911: 'waddell',\n",
              " 52196: '3\\x85',\n",
              " 41188: 'hyperdrive',\n",
              " 34732: 'conditioner',\n",
              " 53153: 'bricklayer',\n",
              " 2576: 'hong',\n",
              " 52198: 'memoriam',\n",
              " 30592: 'inventively',\n",
              " 25249: \"levant's\",\n",
              " 20638: 'portobello',\n",
              " 52200: 'remand',\n",
              " 19504: 'mummified',\n",
              " 27650: 'honk',\n",
              " 19505: 'spews',\n",
              " 40912: 'visitations',\n",
              " 52201: 'mummifies',\n",
              " 25250: 'cavanaugh',\n",
              " 23385: 'zeon',\n",
              " 40913: \"jungle's\",\n",
              " 34733: 'viertel',\n",
              " 27651: 'frenchmen',\n",
              " 52202: 'torpedoes',\n",
              " 52203: 'schlessinger',\n",
              " 34734: 'torpedoed',\n",
              " 69876: 'blister',\n",
              " 52204: 'cinefest',\n",
              " 34735: 'furlough',\n",
              " 52205: 'mainsequence',\n",
              " 40914: 'mentors',\n",
              " 9094: 'academic',\n",
              " 20602: 'stillness',\n",
              " 40915: 'academia',\n",
              " 52206: 'lonelier',\n",
              " 52207: 'nibby',\n",
              " 52208: \"losers'\",\n",
              " 40916: 'cineastes',\n",
              " 4449: 'corporate',\n",
              " 40917: 'massaging',\n",
              " 30593: 'bellow',\n",
              " 19506: 'absurdities',\n",
              " 53241: 'expetations',\n",
              " 40918: 'nyfiken',\n",
              " 75638: 'mehras',\n",
              " 52209: 'lasse',\n",
              " 52210: 'visability',\n",
              " 33946: 'militarily',\n",
              " 52211: \"elder'\",\n",
              " 19023: 'gainsbourg',\n",
              " 20603: 'hah',\n",
              " 13420: 'hai',\n",
              " 34736: 'haj',\n",
              " 25251: 'hak',\n",
              " 4311: 'hal',\n",
              " 4892: 'ham',\n",
              " 53259: 'duffer',\n",
              " 52213: 'haa',\n",
              " 66: 'had',\n",
              " 11930: 'advancement',\n",
              " 16825: 'hag',\n",
              " 25252: \"hand'\",\n",
              " 13421: 'hay',\n",
              " 20604: 'mcnamara',\n",
              " 52214: \"mozart's\",\n",
              " 30731: 'duffel',\n",
              " 30594: 'haq',\n",
              " 13887: 'har',\n",
              " 44: 'has',\n",
              " 2401: 'hat',\n",
              " 40919: 'hav',\n",
              " 30595: 'haw',\n",
              " 52215: 'figtings',\n",
              " 15495: 'elders',\n",
              " 52216: 'underpanted',\n",
              " 52217: 'pninson',\n",
              " 27652: 'unequivocally',\n",
              " 23673: \"barbara's\",\n",
              " 52219: \"bello'\",\n",
              " 12997: 'indicative',\n",
              " 40920: 'yawnfest',\n",
              " 52220: 'hexploitation',\n",
              " 52221: \"loder's\",\n",
              " 27653: 'sleuthing',\n",
              " 32622: \"justin's\",\n",
              " 52222: \"'ball\",\n",
              " 52223: \"'summer\",\n",
              " 34935: \"'demons'\",\n",
              " 52225: \"mormon's\",\n",
              " 34737: \"laughton's\",\n",
              " 52226: 'debell',\n",
              " 39724: 'shipyard',\n",
              " 30597: 'unabashedly',\n",
              " 40401: 'disks',\n",
              " 2290: 'crowd',\n",
              " 10087: 'crowe',\n",
              " 56434: \"vancouver's\",\n",
              " 34738: 'mosques',\n",
              " 6627: 'crown',\n",
              " 52227: 'culpas',\n",
              " 27654: 'crows',\n",
              " 53344: 'surrell',\n",
              " 52229: 'flowless',\n",
              " 52230: 'sheirk',\n",
              " 40923: \"'three\",\n",
              " 52231: \"peterson'\",\n",
              " 52232: 'ooverall',\n",
              " 40924: 'perchance',\n",
              " 1321: 'bottom',\n",
              " 53363: 'chabert',\n",
              " 52233: 'sneha',\n",
              " 13888: 'inhuman',\n",
              " 52234: 'ichii',\n",
              " 52235: 'ursla',\n",
              " 30598: 'completly',\n",
              " 40925: 'moviedom',\n",
              " 52236: 'raddick',\n",
              " 51995: 'brundage',\n",
              " 40926: 'brigades',\n",
              " 1181: 'starring',\n",
              " 52237: \"'goal'\",\n",
              " 52238: 'caskets',\n",
              " 52239: 'willcock',\n",
              " 52240: \"threesome's\",\n",
              " 52241: \"mosque'\",\n",
              " 52242: \"cover's\",\n",
              " 17637: 'spaceships',\n",
              " 40927: 'anomalous',\n",
              " 27655: 'ptsd',\n",
              " 52243: 'shirdan',\n",
              " 21962: 'obscenity',\n",
              " 30599: 'lemmings',\n",
              " 30600: 'duccio',\n",
              " 52244: \"levene's\",\n",
              " 52245: \"'gorby'\",\n",
              " 25255: \"teenager's\",\n",
              " 5340: 'marshall',\n",
              " 9095: 'honeymoon',\n",
              " 3231: 'shoots',\n",
              " 12258: 'despised',\n",
              " 52246: 'okabasho',\n",
              " 8289: 'fabric',\n",
              " 18515: 'cannavale',\n",
              " 3537: 'raped',\n",
              " 52247: \"tutt's\",\n",
              " 17638: 'grasping',\n",
              " 18516: 'despises',\n",
              " 40928: \"thief's\",\n",
              " 8926: 'rapes',\n",
              " 52248: 'raper',\n",
              " 27656: \"eyre'\",\n",
              " 52249: 'walchek',\n",
              " 23386: \"elmo's\",\n",
              " 40929: 'perfumes',\n",
              " 21918: 'spurting',\n",
              " 52250: \"exposition'\\x85\",\n",
              " 52251: 'denoting',\n",
              " 34740: 'thesaurus',\n",
              " 40930: \"shoot'\",\n",
              " 49759: 'bonejack',\n",
              " 52253: 'simpsonian',\n",
              " 30601: 'hebetude',\n",
              " 34741: \"hallow's\",\n",
              " 52254: 'desperation\\x85',\n",
              " 34742: 'incinerator',\n",
              " 10308: 'congratulations',\n",
              " 52255: 'humbled',\n",
              " 5924: \"else's\",\n",
              " 40845: 'trelkovski',\n",
              " 52256: \"rape'\",\n",
              " 59386: \"'chapters'\",\n",
              " 52257: '1600s',\n",
              " 7253: 'martian',\n",
              " 25256: 'nicest',\n",
              " 52259: 'eyred',\n",
              " 9457: 'passenger',\n",
              " 6041: 'disgrace',\n",
              " 52260: 'moderne',\n",
              " 5120: 'barrymore',\n",
              " 52261: 'yankovich',\n",
              " 40931: 'moderns',\n",
              " 52262: 'studliest',\n",
              " 52263: 'bedsheet',\n",
              " 14900: 'decapitation',\n",
              " 52264: 'slurring',\n",
              " 52265: \"'nunsploitation'\",\n",
              " 34743: \"'character'\",\n",
              " 9880: 'cambodia',\n",
              " 52266: 'rebelious',\n",
              " 27657: 'pasadena',\n",
              " 40932: 'crowne',\n",
              " 52267: \"'bedchamber\",\n",
              " 52268: 'conjectural',\n",
              " 52269: 'appologize',\n",
              " 52270: 'halfassing',\n",
              " 57816: 'paycheque',\n",
              " 20606: 'palms',\n",
              " 52271: \"'islands\",\n",
              " 40933: 'hawked',\n",
              " 21919: 'palme',\n",
              " 40934: 'conservatively',\n",
              " 64007: 'larp',\n",
              " 5558: 'palma',\n",
              " 21920: 'smelling',\n",
              " 12998: 'aragorn',\n",
              " 52272: 'hawker',\n",
              " 52273: 'hawkes',\n",
              " 3975: 'explosions',\n",
              " 8059: 'loren',\n",
              " 52274: \"pyle's\",\n",
              " 6704: 'shootout',\n",
              " 18517: \"mike's\",\n",
              " 52275: \"driscoll's\",\n",
              " 40935: 'cogsworth',\n",
              " 52276: \"britian's\",\n",
              " 34744: 'childs',\n",
              " 52277: \"portrait's\",\n",
              " 3626: 'chain',\n",
              " 2497: 'whoever',\n",
              " 52278: 'puttered',\n",
              " 52279: 'childe',\n",
              " 52280: 'maywether',\n",
              " 3036: 'chair',\n",
              " 52281: \"rance's\",\n",
              " 34745: 'machu',\n",
              " 4517: 'ballet',\n",
              " 34746: 'grapples',\n",
              " 76152: 'summerize',\n",
              " 30603: 'freelance',\n",
              " 52283: \"andrea's\",\n",
              " 52284: '\\x91very',\n",
              " 45879: 'coolidge',\n",
              " 18518: 'mache',\n",
              " 52285: 'balled',\n",
              " 40937: 'grappled',\n",
              " 18519: 'macha',\n",
              " 21921: 'underlining',\n",
              " 5623: 'macho',\n",
              " 19507: 'oversight',\n",
              " 25257: 'machi',\n",
              " 11311: 'verbally',\n",
              " 21922: 'tenacious',\n",
              " 40938: 'windshields',\n",
              " 18557: 'paychecks',\n",
              " 3396: 'jerk',\n",
              " 11931: \"good'\",\n",
              " 34748: 'prancer',\n",
              " 21923: 'prances',\n",
              " 52286: 'olympus',\n",
              " 21924: 'lark',\n",
              " 10785: 'embark',\n",
              " 7365: 'gloomy',\n",
              " 52287: 'jehaan',\n",
              " 52288: 'turaqui',\n",
              " 20607: \"child'\",\n",
              " 2894: 'locked',\n",
              " 52289: 'pranced',\n",
              " 2588: 'exact',\n",
              " 52290: 'unattuned',\n",
              " 783: 'minute',\n",
              " 16118: 'skewed',\n",
              " 40940: 'hodgins',\n",
              " 34749: 'skewer',\n",
              " 52291: 'think\\x85',\n",
              " 38765: 'rosenstein',\n",
              " 52292: 'helmit',\n",
              " 34750: 'wrestlemanias',\n",
              " 16826: 'hindered',\n",
              " 30604: \"martha's\",\n",
              " 52293: 'cheree',\n",
              " 52294: \"pluckin'\",\n",
              " 40941: 'ogles',\n",
              " 11932: 'heavyweight',\n",
              " 82190: 'aada',\n",
              " 11312: 'chopping',\n",
              " 61534: 'strongboy',\n",
              " 41342: 'hegemonic',\n",
              " 40942: 'adorns',\n",
              " 41346: 'xxth',\n",
              " 34751: 'nobuhiro',\n",
              " 52298: 'capitães',\n",
              " 52299: 'kavogianni',\n",
              " 13422: 'antwerp',\n",
              " 6538: 'celebrated',\n",
              " 52300: 'roarke',\n",
              " 40943: 'baggins',\n",
              " 31270: 'cheeseburgers',\n",
              " 52301: 'matras',\n",
              " 52302: \"nineties'\",\n",
              " 52303: \"'craig'\",\n",
              " 12999: 'celebrates',\n",
              " 3383: 'unintentionally',\n",
              " 14362: 'drafted',\n",
              " 52304: 'climby',\n",
              " 52305: '303',\n",
              " 18520: 'oldies',\n",
              " 9096: 'climbs',\n",
              " 9655: 'honour',\n",
              " 34752: 'plucking',\n",
              " 30074: '305',\n",
              " 5514: 'address',\n",
              " 40944: 'menjou',\n",
              " 42592: \"'freak'\",\n",
              " 19508: 'dwindling',\n",
              " 9458: 'benson',\n",
              " 52307: 'white’s',\n",
              " 40945: 'shamelessness',\n",
              " 21925: 'impacted',\n",
              " 52308: 'upatz',\n",
              " 3840: 'cusack',\n",
              " 37567: \"flavia's\",\n",
              " 52309: 'effette',\n",
              " 34753: 'influx',\n",
              " 52310: 'boooooooo',\n",
              " 52311: 'dimitrova',\n",
              " 13423: 'houseman',\n",
              " 25259: 'bigas',\n",
              " 52312: 'boylen',\n",
              " 52313: 'phillipenes',\n",
              " 40946: 'fakery',\n",
              " 27658: \"grandpa's\",\n",
              " 27659: 'darnell',\n",
              " 19509: 'undergone',\n",
              " 52315: 'handbags',\n",
              " 21926: 'perished',\n",
              " 37778: 'pooped',\n",
              " 27660: 'vigour',\n",
              " 3627: 'opposed',\n",
              " 52316: 'etude',\n",
              " 11799: \"caine's\",\n",
              " 52317: 'doozers',\n",
              " 34754: 'photojournals',\n",
              " 52318: 'perishes',\n",
              " 34755: 'constrains',\n",
              " 40948: 'migenes',\n",
              " 30605: 'consoled',\n",
              " 16827: 'alastair',\n",
              " 52319: 'wvs',\n",
              " 52320: 'ooooooh',\n",
              " 34756: 'approving',\n",
              " 40949: 'consoles',\n",
              " 52064: 'disparagement',\n",
              " 52322: 'futureistic',\n",
              " 52323: 'rebounding',\n",
              " 52324: \"'date\",\n",
              " 52325: 'gregoire',\n",
              " 21927: 'rutherford',\n",
              " 34757: 'americanised',\n",
              " 82196: 'novikov',\n",
              " 1042: 'following',\n",
              " 34758: 'munroe',\n",
              " 52326: \"morita'\",\n",
              " 52327: 'christenssen',\n",
              " 23106: 'oatmeal',\n",
              " 25260: 'fossey',\n",
              " 40950: 'livered',\n",
              " 13000: 'listens',\n",
              " 76164: \"'marci\",\n",
              " 52330: \"otis's\",\n",
              " 23387: 'thanking',\n",
              " 16019: 'maude',\n",
              " 34759: 'extensions',\n",
              " 52332: 'ameteurish',\n",
              " 52333: \"commender's\",\n",
              " 27661: 'agricultural',\n",
              " 4518: 'convincingly',\n",
              " 17639: 'fueled',\n",
              " 54014: 'mahattan',\n",
              " 40952: \"paris's\",\n",
              " 52336: 'vulkan',\n",
              " 52337: 'stapes',\n",
              " 52338: 'odysessy',\n",
              " 12259: 'harmon',\n",
              " 4252: 'surfing',\n",
              " 23494: 'halloran',\n",
              " 49580: 'unbelieveably',\n",
              " 52339: \"'offed'\",\n",
              " 30607: 'quadrant',\n",
              " 19510: 'inhabiting',\n",
              " 34760: 'nebbish',\n",
              " 40953: 'forebears',\n",
              " 34761: 'skirmish',\n",
              " 52340: 'ocassionally',\n",
              " 52341: \"'resist\",\n",
              " 21928: 'impactful',\n",
              " 52342: 'spicier',\n",
              " 40954: 'touristy',\n",
              " 52343: \"'football'\",\n",
              " 40955: 'webpage',\n",
              " 52345: 'exurbia',\n",
              " 52346: 'jucier',\n",
              " 14901: 'professors',\n",
              " 34762: 'structuring',\n",
              " 30608: 'jig',\n",
              " 40956: 'overlord',\n",
              " 25261: 'disconnect',\n",
              " 82201: 'sniffle',\n",
              " 40957: 'slimeball',\n",
              " 40958: 'jia',\n",
              " 16828: 'milked',\n",
              " 40959: 'banjoes',\n",
              " 1237: 'jim',\n",
              " 52348: 'workforces',\n",
              " 52349: 'jip',\n",
              " 52350: 'rotweiller',\n",
              " 34763: 'mundaneness',\n",
              " 52351: \"'ninja'\",\n",
              " 11040: \"dead'\",\n",
              " 40960: \"cipriani's\",\n",
              " 20608: 'modestly',\n",
              " 52352: \"professor'\",\n",
              " 40961: 'shacked',\n",
              " 34764: 'bashful',\n",
              " 23388: 'sorter',\n",
              " 16120: 'overpowering',\n",
              " 18521: 'workmanlike',\n",
              " 27662: 'henpecked',\n",
              " 18522: 'sorted',\n",
              " 52354: \"jōb's\",\n",
              " 52355: \"'always\",\n",
              " 34765: \"'baptists\",\n",
              " 52356: 'dreamcatchers',\n",
              " 52357: \"'silence'\",\n",
              " 21929: 'hickory',\n",
              " 52358: 'fun\\x97yet',\n",
              " 52359: 'breakumentary',\n",
              " 15496: 'didn',\n",
              " 52360: 'didi',\n",
              " 52361: 'pealing',\n",
              " 40962: 'dispite',\n",
              " 25262: \"italy's\",\n",
              " 21930: 'instability',\n",
              " 6539: 'quarter',\n",
              " 12608: 'quartet',\n",
              " 52362: 'padmé',\n",
              " 52363: \"'bleedmedry\",\n",
              " 52364: 'pahalniuk',\n",
              " 52365: 'honduras',\n",
              " 10786: 'bursting',\n",
              " 41465: \"pablo's\",\n",
              " 52367: 'irremediably',\n",
              " 40963: 'presages',\n",
              " 57832: 'bowlegged',\n",
              " 65183: 'dalip',\n",
              " 6260: 'entering',\n",
              " 76172: 'newsradio',\n",
              " 54150: 'presaged',\n",
              " 27663: \"giallo's\",\n",
              " 40964: 'bouyant',\n",
              " 52368: 'amerterish',\n",
              " 18523: 'rajni',\n",
              " 30610: 'leeves',\n",
              " 34767: 'macauley',\n",
              " 612: 'seriously',\n",
              " 52369: 'sugercoma',\n",
              " 52370: 'grimstead',\n",
              " 52371: \"'fairy'\",\n",
              " 30611: 'zenda',\n",
              " 52372: \"'twins'\",\n",
              " 17640: 'realisation',\n",
              " 27664: 'highsmith',\n",
              " 7817: 'raunchy',\n",
              " 40965: 'incentives',\n",
              " 52374: 'flatson',\n",
              " 35097: 'snooker',\n",
              " 16829: 'crazies',\n",
              " 14902: 'crazier',\n",
              " 7094: 'grandma',\n",
              " 52375: 'napunsaktha',\n",
              " 30612: 'workmanship',\n",
              " 52376: 'reisner',\n",
              " 61306: \"sanford's\",\n",
              " 52377: '\\x91doña',\n",
              " 6108: 'modest',\n",
              " 19153: \"everything's\",\n",
              " 40966: 'hamer',\n",
              " 52379: \"couldn't'\",\n",
              " 13001: 'quibble',\n",
              " 52380: 'socking',\n",
              " 21931: 'tingler',\n",
              " 52381: 'gutman',\n",
              " 40967: 'lachlan',\n",
              " 52382: 'tableaus',\n",
              " 52383: 'headbanger',\n",
              " 2847: 'spoken',\n",
              " 34768: 'cerebrally',\n",
              " 23490: \"'road\",\n",
              " 21932: 'tableaux',\n",
              " 40968: \"proust's\",\n",
              " 40969: 'periodical',\n",
              " 52385: \"shoveller's\",\n",
              " 25263: 'tamara',\n",
              " 17641: 'affords',\n",
              " 3249: 'concert',\n",
              " 87955: \"yara's\",\n",
              " 52386: 'someome',\n",
              " 8424: 'lingering',\n",
              " 41511: \"abraham's\",\n",
              " 34769: 'beesley',\n",
              " 34770: 'cherbourg',\n",
              " 28624: 'kagan',\n",
              " 9097: 'snatch',\n",
              " 9260: \"miyazaki's\",\n",
              " 25264: 'absorbs',\n",
              " 40970: \"koltai's\",\n",
              " 64027: 'tingled',\n",
              " 19511: 'crossroads',\n",
              " 16121: 'rehab',\n",
              " 52389: 'falworth',\n",
              " 52390: 'sequals',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybg0flABcCuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "6b9763de-dfdf-4775-90d6-bb32a62023e9"
      },
      "source": [
        "decoded_review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlGyiAG9cCuA"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "\n",
        "We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:\n",
        "\n",
        "* We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape `(samples, word_indices)`, \n",
        "then use as first layer in our network a layer capable of handling such integer tensors (the `Embedding` layer, which we will cover in \n",
        "detail later in the book).\n",
        "* We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence \n",
        "`[3, 5]` into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as \n",
        "first layer in our network a `Dense` layer, capable of handling floating point vector data.\n",
        "\n",
        "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oExBDAYwcCuA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBsf-1rcCuB"
      },
      "source": [
        "Here's what our samples look like now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yLuHhTdcCuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea373a2-c883-4d2c-d602-68d24a056f48"
      },
      "source": [
        "x_train[0]\n",
        "#first movie data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uNEEveOcCuC"
      },
      "source": [
        "We should also vectorize our labels, which is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ-_6DDccCuC"
      },
      "source": [
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoosh-Kd71O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7991764b-4782-4fe7-9556-69ad4eaba1af"
      },
      "source": [
        "x_train.shape\n",
        "#check the shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwwgl5-ccCuD",
        "outputId": "0c059f25-9f86-4a51-8656-5f4d4a8925fb"
      },
      "source": [
        "train_labels[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbnATq3-cCuE"
      },
      "source": [
        "Now our data is ready to be fed into a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3e9dJwYcCuE"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "\n",
        "Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of \n",
        "network that performs well on such a problem would be a simple stack of fully-connected (`Dense`) layers with `relu` activations: `Dense(16, \n",
        "activation='relu')`\n",
        "\n",
        "The argument being passed to each `Dense` layer (16) is the number of \"hidden units\" of the layer. What's a hidden unit? It's a dimension \n",
        "in the representation space of the layer. You may remember from the previous chapter that each such `Dense` layer with a `relu` activation implements \n",
        "the following chain of tensor operations:\n",
        "\n",
        "`output = relu(dot(W, input) + b)` -> def for the dense layer\n",
        "\n",
        "Having 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, i.e. the dot product with `W` will project the \n",
        "input data onto a 16-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). You can \n",
        "intuitively understand the dimensionality of your representation space as \"how much freedom you are allowing the network to have when \n",
        "learning internal representations\". Having more hidden units (a higher-dimensional representation space) allows your network to learn more \n",
        "complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that \n",
        "will improve performance on the training data but not on the test data).\n",
        "\n",
        "There are two key architecture decisions to be made about such stack of dense layers:\n",
        "\n",
        "* How many layers to use.\n",
        "* How many \"hidden units\" to chose for each layer.\n",
        "\n",
        "In the next chapter, you will learn formal principles to guide you in making these choices. \n",
        "For the time being, you will have to trust us with the following architecture choice: \n",
        "two intermediate layers with 16 hidden units each, \n",
        "and a third layer which will output the scalar prediction regarding the sentiment of the current review. \n",
        "The intermediate layers will use `relu` as their \"activation function\", \n",
        "and the final layer will use a sigmoid activation so as to output a probability \n",
        "(a score between 0 and 1, indicating how likely the sample is to have the target \"1\", i.e. how likely the review is to be positive). \n",
        "A `relu` (rectified linear unit) is a function meant to zero-out negative values, \n",
        "while a sigmoid \"squashes\" arbitrary values into the `[0, 1]` interval, thus outputting something that can be interpreted as a probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx2ny5IlcCuF"
      },
      "source": [
        "Here's what our network looks like:\n",
        "\n",
        "![3-layer network](https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKwvWcCecCuF"
      },
      "source": [
        "And here's the Keras implementation, very similar to the MNIST example you saw previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQSIaN1wcCuF"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) #applying the first dense layer\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmZa-Ak-cCuG"
      },
      "source": [
        "\n",
        "Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network \n",
        "is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the `binary_crossentropy` loss. \n",
        "It isn't the only viable choice: you could use, for instance, `mean_squared_error`. But crossentropy is usually the best choice when you \n",
        "are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the \"distance\" \n",
        "between probability distributions, or in our case, between the ground-truth distribution and our predictions.\n",
        "\n",
        "Here's the step where we configure our model with the `rmsprop` optimizer and the `binary_crossentropy` loss function. Note that we will \n",
        "also monitor accuracy during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JralngDacCuG"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy', #crossentropy because it's classification problem.\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpT2aUr9cCuG"
      },
      "source": [
        "We are passing our optimizer, loss function and metrics as strings, which is possible because `rmsprop`, `binary_crossentropy` and \n",
        "`accuracy` are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss \n",
        "function or metric function. This former can be done by passing an optimizer class instance as the `optimizer` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVK6NvLpcCuG"
      },
      "source": [
        "#proceed training\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7DvD03ycCuH"
      },
      "source": [
        "The latter can be done by passing function objects as the `loss` or `metrics` arguments:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS6Lyx5_cCuH"
      },
      "source": [
        "## Validating our approach\n",
        "\n",
        "In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a \"validation set\" by \n",
        "setting apart 10,000 samples from the original training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhl8EthQcCuH"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "rsQs-3S-cCuH"
      },
      "source": [
        "We will now train our model for 20 epochs (20 iterations over all samples in the `x_train` and `y_train` tensors), in mini-batches of 512 \n",
        "samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the \n",
        "validation data as the `validation_data` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynjZF8n8cCuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5679ab1-024b-42e6-b3aa-c0cf8b583c5c"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 4s 29ms/step - loss: 0.5985 - accuracy: 0.6969 - val_loss: 0.4003 - val_accuracy: 0.8650\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.3370 - accuracy: 0.9022 - val_loss: 0.3090 - val_accuracy: 0.8885\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.2365 - accuracy: 0.9280 - val_loss: 0.2827 - val_accuracy: 0.8884\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.1872 - accuracy: 0.9376 - val_loss: 0.2770 - val_accuracy: 0.8879\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.1389 - accuracy: 0.9617 - val_loss: 0.2905 - val_accuracy: 0.8841\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1152 - accuracy: 0.9681 - val_loss: 0.2946 - val_accuracy: 0.8847\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0954 - accuracy: 0.9739 - val_loss: 0.3141 - val_accuracy: 0.8810\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0785 - accuracy: 0.9800 - val_loss: 0.3362 - val_accuracy: 0.8802\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0636 - accuracy: 0.9853 - val_loss: 0.3513 - val_accuracy: 0.8776\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0496 - accuracy: 0.9888 - val_loss: 0.3775 - val_accuracy: 0.8776\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0421 - accuracy: 0.9913 - val_loss: 0.4105 - val_accuracy: 0.8738\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0308 - accuracy: 0.9948 - val_loss: 0.4321 - val_accuracy: 0.8734\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0249 - accuracy: 0.9961 - val_loss: 0.4650 - val_accuracy: 0.8717\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0176 - accuracy: 0.9983 - val_loss: 0.5574 - val_accuracy: 0.8612\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0153 - accuracy: 0.9980 - val_loss: 0.5649 - val_accuracy: 0.8635\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0135 - accuracy: 0.9990 - val_loss: 0.5602 - val_accuracy: 0.8694\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0082 - accuracy: 0.9993 - val_loss: 0.5964 - val_accuracy: 0.8676\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0061 - accuracy: 0.9998 - val_loss: 0.6243 - val_accuracy: 0.8673\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.6693 - val_accuracy: 0.8652\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.6916 - val_accuracy: 0.8649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2FKgjjtcCuI"
      },
      "source": [
        "On CPU, this will take less than two seconds per epoch -- training is over in 20 seconds. At the end of every epoch, there is a slight pause \n",
        "as the model computes its loss and accuracy on the 10,000 samples of the validation data.\n",
        "\n",
        "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data \n",
        "about everything that happened during training. Let's take a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fO3ryQQcCuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53537e77-006f-4ae4-d1e7-d768bdffe169"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE_8ZQWZcCuJ"
      },
      "source": [
        "It contains 4 entries: one per metric that was being monitored, during training and during validation. Let's use Matplotlib to plot the \n",
        "training and validation loss side by side, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19MlR-xHcCuJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a824c615-975c-4f03-95a1-a041a7e99ef2"
      },
      "source": [
        "#Visualize how the training process grows\n",
        "#training loss decreasing & validation loss increasing => overfitting problem is happenening\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJjwvbCEcCuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5b6426eb-8e4d-4595-d8b9-1eb22540c505"
      },
      "source": [
        "#Accuracy visualization\n",
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqDj-gmycCuL"
      },
      "source": [
        "\n",
        "The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary \n",
        "slightly due to a different random initialization of your network.\n",
        "\n",
        "As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would \n",
        "expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration. But that \n",
        "isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning \n",
        "against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen \n",
        "before. In precise terms, what you are seeing is \"overfitting\": after the second epoch, we are over-optimizing on the training data, and we \n",
        "ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
        "\n",
        "In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can \n",
        "leverage to mitigate overfitting, which we will cover in the next chapter.\n",
        "\n",
        "Let's train a new network from scratch for four epochs, then evaluate it on our test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLE9HIhmcCuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510ae388-85d3-42ce-a067-45c2ca106a21"
      },
      "source": [
        "#Another model_evaluation\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)\n",
        "#As a result, we got 88.36% accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.5490 - accuracy: 0.7484\n",
            "Epoch 2/4\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.2774 - accuracy: 0.9107\n",
            "Epoch 3/4\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.2075 - accuracy: 0.9299\n",
            "Epoch 4/4\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.1668 - accuracy: 0.9439\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2955 - accuracy: 0.8836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUnttWcDcCuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8921f8-a595-4e85-daeb-729707dec71d"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2955377399921417, 0.8835600018501282]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbTnEDnacCuM"
      },
      "source": [
        "Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xufS9wamcCuM"
      },
      "source": [
        "## Using a trained network to generate predictions on new data\n",
        "\n",
        "After having trained a network, you will want to use it in a practical setting. You can generate the likelihood of reviews being positive \n",
        "by using the `predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXPL0W0AcCuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9d7849-e3e8-4fe3-8adc-7f4281c4753f"
      },
      "source": [
        "#predict new data\n",
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17458314],\n",
              "       [0.9977921 ],\n",
              "       [0.89840686],\n",
              "       ...,\n",
              "       [0.06677828],\n",
              "       [0.07408923],\n",
              "       [0.5870187 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjiqlN0PcCuN"
      },
      "source": [
        "As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpRyRM-_cCuN"
      },
      "source": [
        "## Further experiments\n",
        "\n",
        "\n",
        "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
        "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
        "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
        "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.\n",
        "\n",
        "These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be \n",
        "improved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYBB16ItcCuN"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it -- as tensors -- into a neural \n",
        "network. In the case of sequences of words, they can be encoded as binary vectors -- but there are other encoding options too.\n",
        "* Stacks of `Dense` layers with `relu` activations can solve a wide range of problems (including sentiment classification), and you will \n",
        "likely use them frequently.\n",
        "* In a binary classification problem (two output classes), your network should end with a `Dense` layer with 1 unit and a `sigmoid` activation, \n",
        "i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
        "* With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is `binary_crossentropy`.\n",
        "* The `rmsprop` optimizer is generally a good enough choice of optimizer, whatever your problem. That's one less thing for you to worry \n",
        "about.\n",
        "* As they get better on their training data, neural networks eventually start _overfitting_ and end up obtaining increasingly worse results on data \n",
        "never-seen-before. Make sure to always monitor performance on data that is outside of the training set.\n"
      ]
    }
  ]
}